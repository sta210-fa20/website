---
title: "Statistical inference review"
author: "Prof. Maria Tackett"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta210-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
      ratio: "16:9"
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5, dpi = 300, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center") 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
library(emo)
library(openintro)
library(infer)
library(patchwork)
```

---

class: middle center

## [Click for PDF of slides](03-inference-review.pdf)

---


class: center, middle

## Sample Statistics and Sampling Distributions

---

## Variability of sample statistics

- We've seen that each sample from the population yields a slightly different 
sample statistic (sample mean, sample proportion, etc.)

- Previously we've quantified this value via simulation

- Today we talk about some of the theory underlying .vocab[sampling distributions],
particularly as they relate to sample means.

---

## Statistical inference

- Statistical inference is the act of generalizing from a sample in order to 
make conclusions regarding a population. 

- We are interested in population parameters, which we do not observe. Instead, 
we must calculate statistics from our sample in order to learn about them.

- As part of this process, we must quantify the degree of uncertainty in our 
sample statistic. 

---

## Sampling distribution of the mean

Suppose weâ€™re interested in the mean resting heart rate of students at Duke, and 
are able to do the following:

--

1. Take a random sample of size $n$ from this population, and calculate the 
mean resting heart rate in this sample, $\bar{X}_1$

--

2. Put the sample back, take a second random sample of size $n$, and calculate 
the mean resting heart rate from this new sample, $\bar{X}_2$

--

3. Put the sample back, take a third random sample of size $n$, and calculate
the mean resting heart rate from this sample, too...

--

...and so on.

---

## Sampling distribution of the mean

After repeating this many times, we have a dataset that has the
sample averages from the population: $\bar{X}_1$, $\bar{X}_2$, $\cdots$,
$\bar{X}_K$ (assuming we took $K$ total samples).

--

.question[
Can we say anything about the distribution of these sample means (that is, the
.vocab[sampling distribution] of the mean?) 
]

*(Keep in mind, we don't know what the underlying distribution of mean resting 
heart rate looks like in Duke students!)*

---

class: center, middle

## The Central Limit Theorem

---

class: middle

A quick caveat...

For now, let's assume we know the underlying standard deviation, $\sigma$, from our distribution 

---

## The Central Limit Theorem

For a population with a well-defined mean $\mu$ and standard deviation $\sigma$,
these three properties hold for the distribution of sample average $\bar{X}$,
assuming certain conditions hold:

--

1. The mean of the sampling distribution of the mean is identical to the 
population mean $\mu$.

--

2. The standard deviation of the distribution of the sample averages is
$\sigma/\sqrt{n}$.
  - This is called the .vocab[standard error] (SE) of the mean. 
 
--

3. For $n$ large enough, the shape of the
sampling distribution of means is approximately .vocab[normally distributed].

---

## The normal (Gaussian) distribution

The normal distribution is unimodal and symmetric and is described by its
.vocab[density function]:

If a random variable $X$ follows the normal distribution, then
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right\}$$
where $\mu$ is the mean and $\sigma^2$ is the variance $(\sigma \text{ is the standard deviation})$

.alert[
We often write $N(\mu, \sigma)$ to describe this distribution.
]
---

## The normal distribution (graphically)

```{r echo = F, fig.height = 2.5}
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-3, 3), color = "black", size = 1.5) +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Wait, *any* distribution?

The central limit theorem tells us that *<b>sample averages</b>* are normally distributed, if we have enough data and certain assumptions hold. 

This is true *even if our original variables are not normally distributed*.

Click [here](http://onlinestatbook.com/stat_sim/sampling_dist/index.html) to see an interactive demonstration of this idea.

---

## Conditions for CLT 

We need to check two conditions for CLT to hold: independence, sample size/distribution. 

--

`r emo::ji("white_check_mark")` .vocab[Independence:] The sampled observations must be independent. This is 
difficult to check, but the following are useful guidelines:

- the sample must be randomly taken
- if sampling without replacement, sample size must be less than 10% of the 
    population size

--

If samples are independent, then by definition one sample's value does not "influence" another sample's value.

---

## Conditions for CLT 

`r emo::ji("white_check_mark")` .vocab[Sample size / distribution:] 

- if data are numerical, usually n > 30 is considered a large enough sample for the CLT to kick in
- if we know for sure that the underlying data are normally distributed, then the distribution of sample averages will also be exactly normal, regardless of the sample size
- if data are categorical, at least 10 successes and 10 failures.

---

class: middle, center

## Let's run our own simulation

---

### Underlying population (not observed in real life!)

.small[
```{r}
rs_pop <- tibble(x = rbeta(100000, 1, 5) * 100)
```
]

```{r echo=FALSE, fig.height=2}
ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 2, fill = "skyblue", color = "darkblue") +
  labs(title = "Population distribution", x = "") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

**The true population parameters**
.small[
```{r echo = F}
rs_pop %>%
  summarise(mu = mean(x), sigma = sd(x))
```
]

---

## Sampling from the population - 1

```{r}
set.seed(1)
samp_1 <- rs_pop %>%
  sample_n(size = 50) %>%
  summarise(x_bar = mean(x))
```

```{r}
samp_1
```

---

## Sampling from the population - 2

```{r}
set.seed(2)
samp_2 <- rs_pop %>%
  sample_n(size = 50) %>% 
  summarise(x_bar = mean(x))
```

```{r}
samp_2
```
---

## Sampling from the population - 3

```{r}
set.seed(3)
samp_3 <- rs_pop %>%
  sample_n(size = 50) %>% 
  summarise(x_bar = mean(x))
```

```{r}
samp_3
```

--

keep repeating...

---

## Sampling distribution

.small[
```{r}
set.seed(092620)
sampling <- rs_pop %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 5000) %>%
  group_by(replicate) %>%
  summarise(xbar = mean(x))
```
]

```{r echo=FALSE, fig.height=1.9}
ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "darkblue") +
  labs(title = "Sampling distribution of sample means") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

```{r}
sampling %>%
  summarise(mean = mean(xbar), se = sd(xbar))
```

---

.question[
How do the shapes, centers, and spreads of these distributions compare?
]

```{r echo=FALSE, fig.height=2.5}
p1 <- ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 2, fill = "skyblue", color = "darkblue") +
  labs(title = "Population distribution", x = "") +
  xlim(-5, 100) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())

p2 <- ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "darkblue") +
  labs(title = "Sampling distribution of sample means", x = "Sample means") +
  xlim(-5, 100) +
  labs(x = " ", 
       y = " ") +
    theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())

p1 / p2
```

---

## Recap

- If certain assumptions are satisfied, regardless of the shape of the 
population distribution, the sampling distribution of the mean follows an 
approximately normal distribution.

--

- The center of the sampling distribution is at the center of the population 
distribution.

--

- The sampling distribution is less variable than the population distribution 
(and we can quantify by how much).

--

.question[
What is the standard error, and how are the standard error and sample size 
related? What does that say about how the spread of the sampling distribution
changes as $n$ increases?
]

---

class: center, middle

## Finding probabilities in R

---

## Probabilities under N(0,1) curve

```{r}
# P(Z < -1.5)
pnorm(-1.5)
```

```{r echo = F, fig.height = 2}
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-3, -1.5), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-1.5, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

--

```{r echo = F, fig.height = 2}
from.z <- qnorm(pnorm(-1))
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
    geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-3, from.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

##  Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]


```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#C71585", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(pnorm(-1))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#C71585", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r echo = F, fig.height = 2}
from.z <- qnorm(pnorm(-1))
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
    geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-3, from.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

```{r}
pnorm(2) - pnorm(-1)
```

---

##  Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r}
pnorm(2) - pnorm(-1)
```

---

## Finding cutoff values under N(0,1) curve

```{r}
# find Q1
qnorm(0.25)
```


```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(.25)
 
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Looking ahead...

We will use the Central Limit Theorem and the normal distribution to conduct statistical inference.

---
## The Central Limit Theorem

For a population with a well-defined mean $\mu$ and standard deviation $\sigma$,
these three properties hold for the distribution of sample average $\bar{X}$,
assuming certain conditions hold:

`r emo::ji("white_check_mark")` The distribution of the sample statistic is nearly normal

`r emo::ji("white_check_mark")` The distribution is centered at the (often unknown) population parameter

`r emo::ji("white_check_mark")` The variability of the distribution is inversely proportional to the square 
root of the sample size

---

## Why do we care?

Knowing the distribution of the sample statistic $\bar{X}$ can help us

--

- estimate a population parameter as point **estimate** $\boldsymbol{\pm}$ **margin of error**
  - the .vocab[margin of error] is comprised of a measure of how confident we want to be and how variable the sample statistic is
--

<br> 

- test for a population parameter by evaluating how likely it is to obtain to observed sample statistic when assuming that the null hypothesis is true
  - this probability will depend on how variable the sampling distribution is

---

class: center, middle

## Inference based on the CLT

---

## Inference based on the CLT

If necessary conditions are met, we can also use inference methods based on the CLT. Suppose we know the true population standard deviation. 

--

Then the CLT tells us that $\bar{X}$ approximately has the distribution $N\left(\mu, \sigma/\sqrt{n}\right)$. 

That is,

$$Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$

---

## Probabilities under N(0,1) curve

```{r}
# P(Z < -1.5)
pnorm(-1.5)
```

```{r echo = F, fig.height = 2}
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-3, -1.5), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-1.5, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

--

```{r echo = F, fig.height = 2}
from.z <- qnorm(pnorm(-1))
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
    geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-3, from.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

##  Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]


```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#C71585", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(pnorm(-1))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#C71585", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r echo = F, fig.height = 2}
from.z <- qnorm(pnorm(-1))
to.z <- qnorm(pnorm(2))

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
    geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(-3, from.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

```{r}
pnorm(2) - pnorm(-1)
```

---

##  Probability between two values

.question[
If $Z \sim N(0, 1)$, what is $P(-1 < Z < 2)$?
]

```{r}
pnorm(2) - pnorm(-1)
```

---

## Finding cutoff values under N(0,1) curve

```{r}
# find the median, Q2
qnorm(0.5)
```


```{r echo = F, fig.height = 2}
from.z <- -3
to.z <- qnorm(.5)
 
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(from.z, to.z), color = "black") +
  geom_area(stat = "function", fun = dnorm, fill = "#FFFFFF", xlim = c(to.z, 3), color = "black") +
  labs(x = "", 
       y = "Density") +
  theme_bw() +
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3))
```

---

## What if $\sigma$ isn't known?

```{r fig.align="center",out.width="80%",echo=FALSE}
knitr::include_graphics("img/03/guinness.jpg")
```

---

## T distribution

- In practice, we never know the true value of $\sigma$, and so we estimate it from our data with $s$.

We can make the following test statistic for testing
a single sample's population mean, which has a .vocab[t-distribution with n-1 degrees of freedom]:

.question[
$$ T = \frac{\bar{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$$
]

---

## T distribution 


The t-distribution is also unimodal and symmetric, and is centered at 0

--

Thicker tails than the normal distribution
  - This is to make up for additional variability introduced by using $s$ instead of $\sigma$ in calculation of the 
SE

---

## T vs Z distributions

```{r echo=FALSE, fig.height = 2.5}
x = seq(-5,5,len=1000)
d = rbind(
  data.frame(dist="t (df=1)",  x=x, d=dt(x,1)),
  data.frame(dist="t (df=3)",  x=x, d=dt(x,3)),
  data.frame(dist="t (df=5)",  x=x, d=dt(x,5)),
  data.frame(dist="t (df=10)", x=x, d=dt(x,10)),
  data.frame(dist="t (df=15)", x=x, d=dt(x,15)),
  data.frame(dist="t (df=30)", x=x, d=dt(x,30)),
  data.frame(dist="Z", x=x, d=dnorm(x))
)

ggplot(d, aes(x=x, y=d, color=dist)) + geom_line()
```

---

## T distribution

.pull-left[
.vocab[Finding probabilities under the t curve:]

```{r}
#P(t < -1.96)
pt(-1.96, df = 9)
```

```{r}
#P(t > -1.96)
pt(-1.96, df = 9, 
   lower.tail = FALSE)
```
]

--

.pull-right[

.vocab[Finding cutoff values under the t curve:]

```{r}
# Find Q1
qt(0.25, df = 9)
```

```{r}
# Q3
qt(0.75, df = 9)
```
]


---

## Resident satisfaction in Durham

`durham_survey` contains resident responses to a survey given by the City of
Durham in 2018. These are a randomly selected, representative sample of
Durham residents.

Questions were rated 1 - 5, with 1 being "highly dissatisfied" and 5 being
"highly satisfied."

--

.question[
Is there evidence that, on average, Durham residents are generally satisfied (score greater than 3) with the quality of the public library system? 
]

---

## Exploratory Data Analysis

```{r fig.height=2.5, fig.width=5, message=FALSE}
durham <- read_csv("data/durham_survey.csv") %>%
  filter(quality_library != 9)
```

```{r}
durham %>% 
  summarise(x_bar = mean(quality_library), 
            med = median(quality_library), 
            sd = sd(quality_library), n = n())
```

---

## Exploratory Data Analysis

```{r warning = F, echo = F, fig.height = 2.5}
durham %>% 
  filter(quality_library!= 9) %>% 
  ggplot(aes(x = quality_library)) + 
    geom_histogram(binwidth = 0.95) +
    labs(x = "Satisfaction with public library system", y = "Count",
         title = "Most residents are generally satisifed with the public library system")
```

---

## Hypotheses

.question[
What are the hypotheses for evaluating if Durham residents, on average, are generally satisfied with the public library system?
]

--

$$H_0: \mu = 3$$ 
$$H_a: \mu > 3$$

---

## Conditions

.question[
What conditions must be satisfied to conduct this hypothesis test using methods 
based on the CLT? Are these conditions satisfied?
]

--

**Independence?** 

--

`r emo::ji("white_check_mark")` The residents were randomly selected for the survey, and 521 is less than 10% of the Durham population (~ 270,000).

--

**Sample size / distribution?**

--

`r emo::ji("white_check_mark")` 521 > 30, so the sample is large enough to apply the Central Limit Theorem. 

---

## Calculating the test statistic

Summary statistics from the sample:

```{r fig.height=3, fig.width=5, echo=FALSE}
durham_summary <- durham %>% 
  filter(quality_library != 9) %>%
  summarise(xbar = mean(quality_library), s = sd(quality_library), n = n())
durham_summary
```

--

And the CLT says:

$$\bar{x} \sim N\left(mean = \mu, SE = \frac{\sigma}{\sqrt{n}}\right)$$

--

.question[
How many standard errors away from the population mean is the observed sample 
mean?
]

.question[
How likely are we to observe a sample mean that is at least as extreme as the 
observed sample mean, if in fact the null hypothesis is true?
]

---

## Calculations

--

```{r}
(se <- durham_summary$s / sqrt(durham_summary$n)) # SE
```

--

```{r}
(t <- (durham_summary$xbar - 3) / se) # Test statistic
```

--

```{r}
(df <- durham_summary$n - 1) # Degrees of freedom
```

--

```{r}
pt(t, df, lower.tail = FALSE) # P-value, P(T > t |H_0 true)
```

---

## Conclusion

The p-value is very small, so we reject $H_0$.

--

The data provide sufficient evidence at the $\alpha = 0.05$ level that Durham residents, on average, are satisfied with the quality of the public library system.

--

.question[
Would you expect a 95% confidence interval to include 3?
]

---

## Confidence interval for a mean

.alert[
**General form of the confidence interval** 

$$point~estimate \pm critical~value \times SE$$
]

--

.alert[
**Confidence interval for the mean** 

$$\bar{x} \pm t^*_{n-1} \times \frac{s}{\sqrt{n}}$$
]

---

## Calculate 95% confidence interval 

.alert[
$$\bar{x} \pm t^*_{n-1} \times \frac{s}{\sqrt{n}}$$
]

--


```{r}
# Critical value 
t_star <- qt(0.975, df)
```

--

```{r}
# Point estimate 
point_est <- durham_summary$xbar
```

--

```{r}
# Confidence interval
round(point_est + c(-1,1) * t_star * se, 2)
```

---

## Interpret 95% confidence interval 

```{r}
round(point_est + c(-1,1) * t_star * se, 2)
```

.question[
Interpret this interval in context of the data.
]

--

**We are 95% confident that the true mean rating for Durham residents' satisfaction with the library system is between 3.89 and 4.05.**

---

class: middle, center

## Inference with the CLT using `infer`

---

## CLT-based hypothesis testing in `infer`

$$H_0: \mu = 3 \text{ vs }H_a: \mu > 3$$
--

```{r}
durham %>%
  t_test(response = quality_library, 
         mu = 3, 
         alternative = "greater", 
         conf_int = FALSE)
```

---

## CLT-based confidence intervals in `infer`

Calculate a 95% confidence interval for the mean satisfaction rating.
--

```{r}
durham %>%
  t_test(response = quality_library, 
         alternative = "two-sided",
         conf_int = TRUE, conf_level = 0.95) 
```

---


## Other built-in functionality in R

- There are more built in functions for doing some of these tests in R.

- However a learning goal is this course is not to go through an exhaustive list of all CLT based tests and how to implement them

- Instead the goal is to understand how these methods are / are not like the simulation based methods we learned about earlier

--

.question[
What is similar, and what is different, between CLT based test of means vs. simulation based test?
]

